"""
Conditioning cache utilities for efficient caching of conditioning embeddings.

This module provides utilities for creating cache keys from conditioning dictionaries
and managing LRU-style caches for conditioning embeddings to avoid recomputation.
"""

import hashlib
from typing import Dict, Any, Optional
import torch


def create_conditioning_cache_key(cond_dict: dict, uncond_dict: dict | None) -> str:
    """
    Create a deterministic cache key from conditioning dictionaries.
    
    Generates a SHA-512 hash from the contents of conditioning dictionaries
    to enable efficient caching of preprocessing results.
    
    Args:
        cond_dict (dict): Primary conditioning dictionary
        uncond_dict (dict, optional): Unconditional conditioning dictionary for CFG
        
    Returns:
        str: SHA-512 hexadecimal hash string suitable as cache key
        
    Notes:
        - Handles various value types including tensors, primitives, and collections
        - Tensor hashing includes shape, dtype, and content hash for uniqueness
        - Sort keys to ensure deterministic ordering regardless of dict iteration
        - Uses SHA-512 for low collision probability with reasonable performance
    """
    def value_to_key(value: Any) -> str:
        """Convert a value to a string representation suitable for hashing."""
        if value is None:
            return "None"
        if isinstance(value, torch.Tensor):
            # Include tensor metadata and content hash (deterministic)
            content_hash = hashlib.sha512(value.cpu().float().numpy().tobytes()).hexdigest()
            return f"tensor_{tuple(value.shape)}_{value.dtype}_{content_hash}"
        if isinstance(value, (int, float, str, bool)):
            return str(value)
        if isinstance(value, (list, tuple)):
            return f"list_{[value_to_key(v) for v in value]}"
        return f"other_{type(value).__name__}_{value}"
    
    # Sort dictionary items for deterministic ordering
    cond_items = sorted((k, value_to_key(v)) for k, v in cond_dict.items())
    uncond_items = None if uncond_dict is None else sorted((k, value_to_key(v)) for k, v in uncond_dict.items())
    
    # Create cache string and hash it
    cache_string = f"cond:{cond_items}_uncond:{uncond_items}"
    return hashlib.sha512(cache_string.encode()).hexdigest()


class ConditioningCache:
    """
    LRU-style cache for conditioning embeddings.
    
    Provides efficient caching of conditioning embeddings with automatic
    eviction of oldest entries when capacity is exceeded.
    
    Attributes:
        max_size (int): Maximum number of cached entries
        _cache (dict): Internal cache storage
        
    Notes:
        - Uses insertion order for LRU behavior (Python 3.7+ dict ordering)
        - Automatically evicts oldest entries when max_size is reached
        - Thread-safe for single-threaded usage (not thread-safe across threads)
    """
    
    def __init__(self, max_size: int = 32):
        """
        Initialize conditioning cache.
        
        Args:
            max_size (int): Maximum number of entries to cache (default: 32)
        """
        self.max_size = max_size
        self._cache: Dict[str, torch.Tensor] = {}
    
    def get(self, cache_key: str) -> Optional[torch.Tensor]:
        """
        Retrieve cached conditioning tensor.
        
        Args:
            cache_key (str): Cache key generated by create_conditioning_cache_key
            
        Returns:
            torch.Tensor or None: Cached tensor if found, None otherwise
            
        Notes:
            - Moves accessed item to end (most recent) for LRU behavior
        """
        if cache_key in self._cache:
            # Move to end (most recently used)
            value = self._cache.pop(cache_key)
            self._cache[cache_key] = value
            return value
        return None
    
    def put(self, cache_key: str, tensor: torch.Tensor) -> None:
        """
        Store conditioning tensor in cache.
        
        Args:
            cache_key (str): Cache key for the tensor
            tensor (torch.Tensor): Conditioning tensor to cache
            
        Notes:
            - Evicts oldest entry if cache is at capacity
            - Updates existing entry if key already exists
        """
        if cache_key in self._cache:
            # Update existing entry (move to end)
            self._cache.pop(cache_key)
        elif len(self._cache) >= self.max_size:
            # Evict oldest entry (first item)
            oldest_key = next(iter(self._cache))
            del self._cache[oldest_key]
        
        # Add new entry (at end - most recent)
        self._cache[cache_key] = tensor
    
    def clear(self) -> None:
        """Clear all cached entries."""
        self._cache.clear()
    
    def size(self) -> int:
        """Get current number of cached entries."""
        return len(self._cache)
    
    def contains(self, cache_key: str) -> bool:
        """Check if cache contains the given key."""
        return cache_key in self._cache


def prepare_conditioning_with_cache(
    prefix_conditioner,
    cond_dict: dict, 
    uncond_dict: dict | None = None, 
    use_cache: bool = False, 
    cfg_scale: float = 1.0,
    cache: Optional[ConditioningCache] = None
) -> torch.Tensor:
    """
    Prepare conditioning embeddings with optional caching.
    
    Processes conditioning dictionaries through a prefix conditioner with
    optional caching to avoid recomputation of identical inputs.
    
    Args:
        prefix_conditioner: Conditioner instance with __call__ method
        cond_dict (dict): Primary conditioning inputs
        uncond_dict (dict, optional): Unconditional inputs for classifier-free guidance
        use_cache (bool): Whether to use caching (default: False)
        cfg_scale (float): Classifier-free guidance scale (default: 1.0)
        cache (ConditioningCache, optional): Cache instance to use
        
    Returns:
        torch.Tensor: Processed conditioning embeddings
        
    Notes:
        - If cfg_scale == 1.0, only processes conditional inputs
        - If cfg_scale != 1.0, concatenates conditional and unconditional embeddings
        - Cache is only used if use_cache=True and cache instance is provided
        - Creates unconditional dict from required keys if not provided for CFG
    """
    if not use_cache or cache is None:
        # No caching - direct computation
        if cfg_scale == 1.0:
            return prefix_conditioner(cond_dict)
        if uncond_dict is None:
            uncond_dict = {k: cond_dict[k] for k in prefix_conditioner.required_keys}
        return torch.cat([prefix_conditioner(cond_dict), prefix_conditioner(uncond_dict)])
    
    # Check cache first
    cache_key = create_conditioning_cache_key(cond_dict, uncond_dict)
    cached_result = cache.get(cache_key)
    if cached_result is not None:
        return cached_result
    
    # Compute and cache result
    if cfg_scale == 1.0:
        conditioning = prefix_conditioner(cond_dict)
    else:
        if uncond_dict is None:
            uncond_dict = {k: cond_dict[k] for k in prefix_conditioner.required_keys}
        conditioning = torch.cat([prefix_conditioner(cond_dict), prefix_conditioner(uncond_dict)])
    
    cache.put(cache_key, conditioning)
    return conditioning
